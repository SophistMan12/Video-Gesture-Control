import cv2  # type: ignore
import mediapipe as mp  # type: ignore
import numpy as np  # type: ignore
import os
import time
from keras.models import load_model  # type: ignore

# Load m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
model = load_model(r'D:\VideoGestureControl\gesture_model_v7.h5')

# Nh√£n t∆∞∆°ng ·ª©ng v·ªõi output c·ªßa m√¥ h√¨nh
gesture_labels = [ 'Next', 'Pause', 'Play', 'Start']

# Kh·ªüi t·∫°o MediaPipe
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)

# Chuy·ªÉn landmarks th√†nh vector 42 chi·ªÅu (x, y)
def landmarks_to_vector(hand_landmarks):
    vector = []
    for lm in hand_landmarks.landmark[:21]:  # Ch·ªâ l·∫•y 21 ƒëi·ªÉm ƒë·∫ßu ti√™n
        vector.extend([lm.x, lm.y])
    return np.array(vector, dtype=np.float32)

# D·ª± ƒëo√°n c·ª≠ ch·ªâ t·ª´ landmark
# Th√™m debug ƒë·ªÉ ki·ªÉm tra d·ª± ƒëo√°n

def predict_gesture(hand_landmarks):
    if hand_landmarks is None:
        return None  # Tr·∫£ v·ªÅ None n·∫øu kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c

    input_vector = landmarks_to_vector(hand_landmarks).reshape(1, 42)  # ƒê·∫£m b·∫£o reshape th√†nh (1, 42)
    predictions = model.predict(input_vector, verbose=0)
    #print(f"üîç Raw predictions: {predictions}")  # Debug: In ra d·ª± ƒëo√°n th√¥

    predicted_index = np.argmax(predictions)
    gesture = gesture_labels[predicted_index]
    print(f"‚úÖ Gesture: {gesture} (Index: {predicted_index})")

    with open("gesture_log.txt", "a") as log:
        log.write(f"{time.ctime()}: {gesture}\n")
    return gesture

# Ph√°t video v·ªõi ƒëi·ªÅu khi·ªÉn b·∫±ng c·ª≠ ch·ªâ
def play_video(path):
    global gesture_state, video_index
    video = cv2.VideoCapture(path)

    while True:
        if gesture_state == "Pause":
            cv2.waitKey(100)
        else:
            ret, frame = video.read()
            if not ret:
                print("üîÅ Replaying video")
                video.set(cv2.CAP_PROP_POS_FRAMES, 0)
                continue
            cv2.imshow("Video Player", frame)

        ret_cam, frame_cam = cap.read()
        if not ret_cam:
            print("‚ùå Cannot read webcam.")
            break

        frame_cam = cv2.flip(frame_cam, 1)
        rgb = cv2.cvtColor(frame_cam, cv2.COLOR_BGR2RGB)
        result = hands.process(rgb)

        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                mp_drawing.draw_landmarks(frame_cam, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                gesture = predict_gesture(hand_landmarks)
                if gesture == "Start":
                    gesture_state = "Play"
                elif gesture == "Play":
                    gesture_state = "Play"
                elif gesture == "Pause":
                    gesture_state = "Pause"
                elif gesture == "Next":
                    video_index = (video_index + 1) % len(video_list)
                    video.release()
                    return  # chuy·ªÉn video m·ªõi

        cv2.putText(frame_cam, f"Gesture: {gesture_state}", (10, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)
        cv2.imshow("Gesture Camera", frame_cam)

        key = cv2.waitKey(10) & 0xFF
        if key == 27:  # ESC
            cap.release()
            video.release()
            cv2.destroyAllWindows()
            exit()
        elif key == ord('n'):
            video_index = (video_index + 1) % len(video_list)
            video.release()
            return  # chuy·ªÉn video m·ªõi

# T·∫£i danh s√°ch video
video_folder = r'D:\VideoGestureControl\Video'
video_list = [os.path.join(video_folder, f) for f in os.listdir(video_folder) if f.endswith('.mp4')]

# Kh·ªüi t·∫°o webcam
cap = cv2.VideoCapture(0)
gesture_state = "Pause"
video_index = 0

# V√≤ng l·∫∑p ch√≠nh
while True:
    print(f"‚ñ∂ Playing video: {video_list[video_index]}")
    play_video(video_list[video_index])